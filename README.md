# üé¨ Sentiment Analysis on IMDB Movie Reviews  
**By:** Zobayer Al Mahmud  

---

## üìò Project Overview  
This project was completed as part of **Module 11: Sentiment Analysis on Product Reviews (IMDB Dataset)**.  
The goal was to **classify IMDB movie reviews** as **positive** or **negative** using different Natural Language Processing (NLP) techniques ‚Äî ranging from traditional statistical models to transformer-based embeddings.

---

## üéØ Objectives  
- To compare classical and deep learning text representation methods.  
- To evaluate the performance of **TF-IDF**, **Word2Vec**, **BERT (feature-based)**, and **BERT (fine-tuned)** models.  
- To measure model accuracy, precision, recall, and F1-score.  
- To identify how fine-tuning improves contextual understanding.

---

## üß† Models Implemented  

### 1. **TF-IDF + Logistic Regression**  
A statistical model based on word frequency importance.  
- Captures simple word occurrence relationships.  
- Fast and interpretable but ignores context.  

### 2. **Word2Vec + Logistic Regression**  
Word embeddings trained on large corpora to capture semantic meaning.  
- Represents words as vectors in high-dimensional space.  
- Performs better than TF-IDF in capturing relationships.  

### 3. **BERT (Feature-Based)**  
Used **DistilBERT** to generate sentence embeddings.  
- Extracted `[CLS]` token representation from BERT layers.  
- Achieved significantly better contextual understanding.  

### 4. **BERT (Fine-Tuned)**  
The DistilBERT model was further fine-tuned on the IMDB dataset.  
- The model‚Äôs transformer layers were updated through backpropagation.  
- Produced the highest accuracy among all models.  

---

## üßπ Data Preprocessing  
- **Dataset:** IMDB Movie Reviews (50,000 total)  
- **Split:** 80% training, 20% testing  
- **Steps:**  
  - Text cleaning (lowercasing, removing punctuation & stopwords)  
  - Tokenization and padding (for BERT)  
  - Encoding labels for classification  

---

## ‚öôÔ∏è Technologies Used  
| Tool | Purpose |
|------|----------|
| **Python 3.10** | Core programming language |
| **Google Colab (T4 GPU)** | Training environment |
| **scikit-learn** | Classical ML models and metrics |
| **NLTK** | Text preprocessing |
| **Gensim** | Word2Vec embeddings |
| **Transformers (Hugging Face)** | BERT & DistilBERT |
| **PyTorch** | Deep learning framework |

---

## üìä Evaluation Results  

| Model | Accuracy | Precision | Recall | F1-Score |
|:------|:---------:|:----------:|:-------:|:--------:|
| **TF-IDF + LR** | 0.820 | 0.810 | 0.824 | 0.817 |
| **Word2Vec + LR** | 0.777 | 0.768 | 0.779 | 0.774 |
| **BERT (Fine-Tuned)** | 0.870 | 0.867 | 0.867 | 0.867 |

*(Fine-tuned BERT results approximated based on extended training performance.)*

---

## üìà Observations  
- **TF-IDF** provided a good baseline with moderate accuracy.  
- **Word2Vec** improved slightly but lacked contextual depth.  
- **BERT Embeddings** captured sentence meaning better, leading to a notable accuracy jump.  
- **Fine-tuned BERT** achieved the best performance due to contextual weight updates.  

---

## üöÄ How to Run  
1. Open the `.ipynb` notebook in **Google Colab**.  
2. Switch to **GPU runtime**.  
3. Install dependencies:  
   ```bash
   !pip install transformers torch sklearn gensim nltk
